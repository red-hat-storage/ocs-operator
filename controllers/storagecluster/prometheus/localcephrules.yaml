apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: rook-prometheus
    role: alert-rules
  name: prometheus-ceph-rules
  namespace: rook-ceph
spec:
  groups:
  - name: ceph.rules
    rules:
    - expr: |
        kube_node_status_condition{condition="Ready",job="kube-state-metrics",status="true"} * on (node) group_right() max by (node, namespace) (label_replace(ceph_disk_occupation{job="rook-ceph-mgr"},"node","$1","instance","(.*)"))
      record: cluster:ceph_node_down:join_kube
    - expr: |
        avg by (namespace, managedBy) (topk by (ceph_daemon, namespace, managedBy) (1, label_replace(ceph_disk_occupation{job="rook-ceph-mgr"}, "device", "$1", "device", "/dev/(.*)")) * on(instance, device) group_right(ceph_daemon, namespace, managedBy) topk by (instance, device, namespace, managedBy) (1,(irate(node_disk_read_time_seconds_total[1m]) + irate(node_disk_write_time_seconds_total[1m]) / (clamp_min(irate(node_disk_reads_completed_total[1m]), 1) + irate(node_disk_writes_completed_total[1m])))))
      record: cluster:ceph_disk_latency:join_ceph_node_disk_irate1m
  - name: telemeter.rules
    rules:
    - expr: |
        count by (namespace, managedBy) (ceph_osd_metadata{job=~"rook-ceph-mgr|rook-ceph-mgr-external"})
      record: job:ceph_osd_metadata:count
    - expr: |
        count by (namespace) (kube_persistentvolume_info * on (storageclass) group_left(provisioner, namespace) kube_storageclass_info {provisioner=~"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)|(.*topolvm.cybozu.com)"})
      record: job:odf_system_pvs:count
    - expr: |
        sum by (namespace, managedBy) (ceph_pool_rd{job=~"rook-ceph-mgr|rook-ceph-mgr-external"} + on(pool_id, namespace, managedBy) ceph_pool_wr)
      record: job:ceph_pools_iops:total
    - expr: |
        sum by (namespace, managedBy) (ceph_pool_rd_bytes{job=~"rook-ceph-mgr|rook-ceph-mgr-external"} + on(pool_id, namespace, managedBy) ceph_pool_wr_bytes)
      record: job:ceph_pools_iops_bytes:total
    - expr: |
        count by (namespace, managedBy) (count by (ceph_version, namespace, managedBy) (ceph_mon_metadata{job=~"rook-ceph-mgr|rook-ceph-mgr-external"} or ceph_osd_metadata{job=~"rook-ceph-mgr|rook-ceph-mgr-external"} or ceph_rgw_metadata{job=~"rook-ceph-mgr|rook-ceph-mgr-external"} or ceph_mds_metadata{job=~"rook-ceph-mgr|rook-ceph-mgr-external"} or ceph_mgr_metadata{job=~"rook-ceph-mgr|rook-ceph-mgr-external"}))
      record: job:ceph_versions_running:count
  - name: ceph-mgr-status
    rules:
    - alert: CephMgrIsAbsent
      annotations:
        description: Ceph Manager has disappeared from Prometheus target discovery.
        message: Storage metrics collector service not available anymore.
        severity_level: critical
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephMgrIsAbsent.md
      expr: |
        label_replace((up{job="rook-ceph-mgr"} == 0 or absent(up{job="rook-ceph-mgr"})), "namespace", "openshift-storage", "", "")
      for: 5m
      labels:
        severity: critical
    - alert: CephMgrIsMissingReplicas
      annotations:
        description: Ceph Manager is missing replicas.
        message: Storage metrics collector service doesn't have required no of replicas.
        severity_level: warning
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephMgrIsMissingReplicas.md
      expr: |
        sum by (namespace) (kube_deployment_spec_replicas{deployment=~"rook-ceph-mgr-.*"}) < 1
      for: 5m
      labels:
        severity: warning
  - name: ceph-mds-status
    rules:
    - alert: CephMdsMissingReplicas
      annotations:
        description: Minimum required replicas for storage metadata service not available. Might affect the working of storage cluster.
        message: Insufficient replicas for storage metadata service.
        severity_level: warning
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephMdsMissingReplicas.md
      expr: |
        sum by (namespace, managedBy) (ceph_mds_metadata{job=~"rook-ceph-mgr|rook-ceph-mgr-external"} == 1) < 2
      for: 5m
      labels:
        severity: warning
  - name: quorum-alert.rules
    rules:
    - alert: CephMonQuorumAtRisk
      annotations:
        description: Storage cluster quorum is low. Contact Support.
        message: Storage quorum at risk
        severity_level: error
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephMonQuorumAtRisk.md
      expr: |
        count by (namespace, managedBy) (ceph_mon_quorum_status{job=~"rook-ceph-mgr|rook-ceph-mgr-external"} == 1) <= (floor(count by (namespace, managedBy) (ceph_mon_metadata{job=~"rook-ceph-mgr|rook-ceph-mgr-external"}) / 2) + 1)
      for: 15m
      labels:
        severity: critical
    - alert: CephMonQuorumLost
      annotations:
        description: Storage cluster quorum is lost. Contact Support.
        message: Storage quorum is lost
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephMonQuorumLost.md
        severity_level: critical
        storage_type: ceph
      expr: |
        count by (namespace) (kube_pod_status_phase{pod=~"rook-ceph-mon-.*", phase=~"Running|running"} == 1) < 2
      for: 5m
      labels:
        severity: critical
    - alert: CephMonHighNumberOfLeaderChanges
      annotations:
        description: Ceph Monitor {{ $labels.ceph_daemon }} on host {{ $labels.hostname }} has seen {{ $value | printf "%.2f" }} leader changes per minute recently.
        message: Storage Cluster has seen many leader changes recently.
        severity_level: warning
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephMonHighNumberOfLeaderChanges.md
      expr: |
        (ceph_mon_metadata{job=~"rook-ceph-mgr|rook-ceph-mgr-external"} * on (ceph_daemon, namespace, managedBy) group_left() (rate(ceph_mon_num_elections{job="rook-ceph-exporter"}[5m]) * 60)) > 0.95
      for: 5m
      labels:
        severity: warning
  - name: ceph-node-alert.rules
    rules:
    - alert: CephNodeDown
      annotations:
        description: Storage node {{ $labels.node }} went down. Please check the node immediately.
        message: Storage node {{ $labels.node }} went down
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephNodeDown.md
        severity_level: error
        storage_type: ceph
      expr: |
        cluster:ceph_node_down:join_kube == 0
      for: 30s
      labels:
        severity: critical
  - name: osd-alert.rules
    rules:
    - alert: CephOSDCriticallyFull
      annotations:
        description: Utilization of storage device {{ $labels.ceph_daemon }} of device_class type {{ $labels.device_class }} has crossed 80% on host {{ $labels.hostname }}. Immediately free up some space or add capacity of type {{ $labels.device_class }}.
        message: Back-end storage device is critically full.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephOSDCriticallyFull.md
        severity_level: error
        storage_type: ceph
      expr: |
        (ceph_osd_metadata * on (ceph_daemon, namespace, managedBy) group_right(device_class,hostname) (ceph_osd_stat_bytes_used / ceph_osd_stat_bytes)) >= 0.80
      for: 40s
      labels:
        severity: critical
    - alert: CephOSDFlapping
      annotations:
        description: Storage daemon {{ $labels.ceph_daemon }} has restarted 10 times in last 5 minutes. Please check the pod events or ceph status to find out the cause.
        message: Ceph storage osd flapping.
        severity_level: error
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephOSDFlapping.md
      expr: |
        changes(ceph_osd_up[5m]) >= 10
      for: 0s
      labels:
        severity: critical
    - alert: CephOSDNearFull
      annotations:
        description: Utilization of storage device {{ $labels.ceph_daemon }} of device_class type {{ $labels.device_class }} has crossed 75% on host {{ $labels.hostname }}. Immediately free up some space or add capacity of type {{ $labels.device_class }}.
        message: Back-end storage device is nearing full.
        severity_level: warning
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephOSDNearFull.md
      expr: |
        (ceph_osd_metadata * on (ceph_daemon, namespace, managedBy) group_right(device_class,hostname) (ceph_osd_stat_bytes_used / ceph_osd_stat_bytes)) >= 0.75
      for: 40s
      labels:
        severity: warning
    - alert: CephOSDDiskNotResponding
      annotations:
        description: Disk device {{ $labels.device }} not responding, on host {{ $labels.host }}.
        message: Disk not responding
        severity_level: error
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephOSDDiskNotResponding.md
      expr: |
        label_replace((ceph_osd_in == 1 and ceph_osd_up == 0),"disk","$1","ceph_daemon","osd.(.*)") + on(ceph_daemon, namespace, managedBy) group_left(host, device) label_replace(ceph_disk_occupation{job=~"rook-ceph-mgr|rook-ceph-mgr-external"},"host","$1","instance","(.*)")
      for: 15m
      labels:
        severity: critical
    - alert: CephOSDDiskUnavailable
      annotations:
        description: Disk device {{ $labels.device }} not accessible on host {{ $labels.host }}.
        message: Disk not accessible
        severity_level: error
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephOSDDiskUnavailable.md
      expr: |
        label_replace((ceph_osd_in == 0 and ceph_osd_up == 0),"disk","$1","ceph_daemon","osd.(.*)") + on(ceph_daemon, namespace, managedBy) group_left(host, device) label_replace(ceph_disk_occupation{job=~"rook-ceph-mgr|rook-ceph-mgr-external"},"host","$1","instance","(.*)")
      for: 1m
      labels:
        severity: critical
    - alert: CephOSDDown
      annotations:
        description: Ceph OSD(s) is/are down, please check the ceph-osd daemons and take corrective measures. Please see runbook URL for more details
        message: An OSD has been marked down
        severity_level: warning
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephOSDDown.md
      expr: |
        ceph_health_detail{name="OSD_DOWN"} == 1
      for: 5m
      labels:
        severity: warning
    - alert: CephOSDSlowOps
      annotations:
        description: '{{ $value }} Ceph OSD requests are taking too long to process. Please check ceph status to find out the cause.'
        message: OSD requests are taking too long to process.
        severity_level: warning
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephOSDSlowOps.md
      expr: |
        ceph_healthcheck_slow_ops > 0
      for: 30s
      labels:
        severity: warning
    - alert: CephDataRecoveryTakingTooLong
      annotations:
        description: Data recovery has been active for too long. Contact Support.
        message: Data recovery is slow
        severity_level: warning
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephDataRecoveryTakingTooLong.md
      expr: |
        ceph_pg_undersized > 0
      for: 2h
      labels:
        severity: warning
    - alert: CephPGRepairTakingTooLong
      annotations:
        description: Self heal operations taking too long. Contact Support.
        message: Self heal problems detected
        severity_level: warning
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephPgRepairTakingTooLong.md
      expr: |
        ceph_pg_inconsistent > 0
      for: 1h
      labels:
        severity: warning
  - name: persistent-volume-alert.rules
    rules:
    - alert: PersistentVolumeUsageNearFull
      annotations:
        description: PVC {{ $labels.persistentvolumeclaim }} utilization has crossed 90%. Free up some space or expand the PVC.
        message: PVC {{ $labels.persistentvolumeclaim }} is nearing full. Data deletion or PVC expansion is required.
        severity_level: warning
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/PersistentVolumeUsageNearFull.md
      expr: |
        (kubelet_volume_stats_used_bytes * on (namespace,persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info * on (storageclass)  group_left(provisioner) kube_storageclass_info {provisioner=~"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)"})) / (kubelet_volume_stats_capacity_bytes * on (namespace,persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info * on (storageclass)  group_left(provisioner) kube_storageclass_info {provisioner=~"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)"})) > 0.90
      for: 5s
      labels:
        severity: warning
    - alert: PersistentVolumeUsageCritical
      annotations:
        description: PVC {{ $labels.persistentvolumeclaim }} utilization has crossed 95%. Free up some space or expand the PVC immediately.
        message: PVC {{ $labels.persistentvolumeclaim }} is critically full. Data deletion or PVC expansion is required.
        severity_level: error
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/PersistentVolumeUsageCritical.md
      expr: |
        (kubelet_volume_stats_used_bytes * on (namespace,persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info * on (storageclass)  group_left(provisioner) kube_storageclass_info {provisioner=~"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)"})) / (kubelet_volume_stats_capacity_bytes * on (namespace,persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info * on (storageclass)  group_left(provisioner) kube_storageclass_info {provisioner=~"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)"})) > 0.95
      for: 5s
      labels:
        severity: critical
  - name: cluster-state-alert.rules
    rules:
    - alert: CephClusterErrorState
      annotations:
        description: Storage cluster is in error state for more than 10m.
        message: Storage cluster is in error state
        severity_level: error
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephClusterErrorState.md
      expr: |
        ceph_health_status{job="rook-ceph-mgr"} > 1
      for: 10m
      labels:
        severity: critical
    - alert: CephClusterWarningState
      annotations:
        description: Storage cluster is in warning state for more than 15m.
        message: Storage cluster is in degraded state
        severity_level: warning
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephClusterWarningState.md
      expr: |
        ceph_health_status{job="rook-ceph-mgr"} == 1
      for: 15m
      labels:
        severity: warning
    - alert: CephOSDVersionMismatch
      annotations:
        description: There are {{ $value }} different versions of Ceph OSD components running.
        message: There are multiple versions of storage services running.
        severity_level: warning
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephOSDVersionMismatch.md
      expr: |
        count by (namespace, managedBy) (count by (ceph_version, namespace, managedBy) (ceph_osd_metadata{job="rook-ceph-mgr|rook-ceph-mgr-external", ceph_version != ""})) > 1
      for: 10m
      labels:
        severity: warning
    - alert: CephMonVersionMismatch
      annotations:
        description: There are {{ $value }} different versions of Ceph Mon components running.
        message: There are multiple versions of storage services running.
        severity_level: warning
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephMonVersionMismatch.md
      expr: |
        count by (namespace, managedBy) (count by (ceph_version, namespace, managedBy) (ceph_mon_metadata{job="rook-ceph-mgr|rook-ceph-mgr-external", ceph_version != ""})) > 1
      for: 10m
      labels:
        severity: warning
  - name: cluster-utilization-alert.rules
    rules:
    - alert: CephClusterNearFull
      annotations:
        description: Storage cluster utilization has crossed 75% and will become read-only at 85%. Free up some space or expand the storage cluster.
        message: Storage cluster is nearing full. Data deletion or cluster expansion is required.
        severity_level: warning
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephClusterNearFull.md
      expr: |
        ceph_cluster_total_used_raw_bytes / ceph_cluster_total_bytes > 0.75
      for: 5s
      labels:
        severity: warning
    - alert: CephClusterCriticallyFull
      annotations:
        description: Storage cluster utilization has crossed 80% and will become read-only at 85%. Free up some space or expand the storage cluster immediately.
        message: Storage cluster is critically full and needs immediate data deletion or cluster expansion.
        severity_level: error
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephClusterCriticallyFull.md
      expr: |
        ceph_cluster_total_used_raw_bytes / ceph_cluster_total_bytes > 0.80
      for: 5s
      labels:
        severity: critical
    - alert: CephClusterReadOnly
      annotations:
        description: Storage cluster utilization has crossed 85% and will become read-only now. Free up some space or expand the storage cluster immediately.
        message: Storage cluster is read-only now and needs immediate data deletion or cluster expansion.
        severity_level: error
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephClusterReadOnly.md
      expr: |
        ceph_cluster_total_used_raw_bytes / ceph_cluster_total_bytes >= 0.85
      for: 0s
      labels:
        severity: critical
  - name: pool-quota.rules
    rules:
    - alert: CephPoolQuotaBytesNearExhaustion
      annotations:
        description: Storage pool {{ $labels.name }} quota usage has crossed 70%.
        message: Storage pool quota(bytes) is near exhaustion.
        severity_level: warning
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephPoolQuotaBytesNearExhaustion.md
      expr: |
        (ceph_pool_stored_raw * on (pool_id, managedBy) group_left(name, namespace)ceph_pool_metadata) / ((ceph_pool_quota_bytes * on (pool_id, managedBy) group_left(name, namespace)ceph_pool_metadata) > 0) > 0.70
      for: 1m
      labels:
        severity: warning
    - alert: CephPoolQuotaBytesCriticallyExhausted
      annotations:
        description: Storage pool {{ $labels.name }} quota usage has crossed 90%.
        message: Storage pool quota(bytes) is critically exhausted.
        severity_level: critical
        storage_type: ceph
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephPoolQuotaBytesCriticallyExhausted.md
      expr: |
        (ceph_pool_stored_raw * on (pool_id, managedBy) group_left(name, namespace)ceph_pool_metadata) / ((ceph_pool_quota_bytes * on (pool_id, managedBy) group_left(name, namespace)ceph_pool_metadata) > 0) > 0.90
      for: 1m
      labels:
        severity: critical
