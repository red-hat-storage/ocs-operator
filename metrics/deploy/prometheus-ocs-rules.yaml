apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: prometheus-ocs-rules
  namespace: openshift-storage
spec:
  groups:
  - name: ocs_performance.rules
    rules:
    - expr: "sum by (namespace, managedBy) (\n    topk by (ceph_daemon) (1, label_replace(label_replace(ceph_disk_occupation{job=\"rook-ceph-mgr\"},
        \"instance\", \"$1\", \"exported_instance\", \"(.*)\"), \"device\", \"$1\",
        \"device\", \"/dev/(.*)\")) \n  * on(instance, device) group_left topk by
        (instance,device) \n    (1,\n      (\n        rate(node_disk_read_time_seconds_total[1m])
        / (clamp_min(rate(node_disk_reads_completed_total[1m]), 1))\n      )\n    )\n)\n"
      record: cluster:ceph_disk_latency_read:join_ceph_node_disk_rate1m
    - expr: "sum by (namespace, managedBy) (\n    topk by (ceph_daemon) (1, label_replace(label_replace(ceph_disk_occupation{job=\"rook-ceph-mgr\"},
        \"instance\", \"$1\", \"exported_instance\", \"(.*)\"), \"device\", \"$1\",
        \"device\", \"/dev/(.*)\")) \n  * on(instance, device) group_left topk by
        (instance,device) \n    (1,\n      (\n        rate(node_disk_write_time_seconds_total[1m])
        / (clamp_min(rate(node_disk_writes_completed_total[1m]), 1))\n      )\n    )\n)\n"
      record: cluster:ceph_disk_latency_write:join_ceph_node_disk_rate1m
  - name: ODF_standardized_metrics.rules
    rules:
    - expr: |
        ceph_health_status
      labels:
        system_type: OCS
        system_vendor: Red Hat
      record: odf_system_health_status
    - expr: |
        ceph_cluster_total_bytes
      labels:
        system_type: OCS
        system_vendor: Red Hat
      record: odf_system_raw_capacity_total_bytes
    - expr: |
        ceph_cluster_total_used_raw_bytes
      labels:
        system_type: OCS
        system_vendor: Red Hat
      record: odf_system_raw_capacity_used_bytes
    - expr: |
        sum by (namespace, managedBy, job, service) (rate(ceph_pool_wr[1m]) + rate(ceph_pool_rd[1m]))
      labels:
        system_type: OCS
        system_vendor: Red Hat
      record: odf_system_iops_total_bytes
    - expr: |
        sum by (namespace, managedBy, job, service) (rate(ceph_pool_wr_bytes[1m]) + rate(ceph_pool_rd_bytes[1m]))
      labels:
        system_type: OCS
        system_vendor: Red Hat
      record: odf_system_throughput_total_bytes
    - expr: "sum by (namespace, managedBy, job, service)\n(\n  topk by (ceph_daemon)
        (1, label_replace(label_replace(ceph_disk_occupation{job=\"rook-ceph-mgr\"},
        \"instance\", \"$1\", \"exported_instance\", \"(.*)\"), \"device\", \"$1\",
        \"device\", \"/dev/(.*)\")) \n  * on(instance, device) group_left() topk by
        (instance,device) \n  (1,\n    (\n      (  \n          rate(node_disk_read_time_seconds_total[1m])
        / (clamp_min(rate(node_disk_reads_completed_total[1m]), 1))\n      ) +\n      (\n
        \         rate(node_disk_write_time_seconds_total[1m]) / (clamp_min(rate(node_disk_writes_completed_total[1m]),
        1))\n      )\n    )\n  )\n)\n"
      labels:
        system_type: OCS
        system_vendor: Red Hat
      record: odf_system_latency_seconds
    - expr: |
        sum (ocs_objectbucket_objects_total)
      labels:
        system_type: OCS
        system_vendor: Red Hat
      record: odf_system_objects_total
    - expr: |
        sum (ocs_objectbucket_count_total)
      labels:
        system_type: OCS
        system_vendor: Red Hat
      record: odf_system_bucket_count
  - name: mirroring-alert.rules
    rules:
    - alert: OdfMirrorDaemonStatus
      annotations:
        description: Mirror daemon is in unhealthy status for more than 1m. Mirroring
          is not working as expected in namespace:cluster {{ $labels.namespace }}:{{
          $labels.managedBy }}.
        message: Mirror daemon is unhealthy in namespace:cluster {{ $labels.namespace
          }}:{{ $labels.managedBy }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/OdfMirrorDaemonStatus.md
        severity_level: error
        storage_type: ceph
      expr: |
        ((count by(namespace, managedBy) (ocs_mirror_daemon_count{job="ocs-metrics-exporter"} == 0)) * on(namespace, managedBy) group_left() (count by(namespace, managedBy) (ocs_pool_mirroring_status{job="ocs-metrics-exporter"} == 1))) > 0
      for: 1m
      labels:
        severity: critical
    - alert: OdfPoolMirroringImageHealth
      annotations:
        description: Mirroring image(s) (PV) in the pool {{ $labels.name }} are in
          Unknown state for more than 1m. Mirroring might not work as expected in
          namespace:cluster {{ $labels.namespace }}:{{ $labels.managedBy }}.
        message: Mirroring image(s) (PV) in the pool {{ $labels.name }} are in Unknown
          state in namespace:cluster {{ $labels.namespace }}:{{ $labels.managedBy
          }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/OdfPoolMirroringImageHealth.md
        severity_level: warning
        storage_type: ceph
      expr: |
        (ocs_pool_mirroring_image_health{job="ocs-metrics-exporter"}  * on (namespace, managedBy) group_left() (max by(namespace, managedBy) (ocs_pool_mirroring_status{job="ocs-metrics-exporter"}))) == 1
      for: 1m
      labels:
        mirroring_image_status: unknown
        severity: warning
    - alert: OdfPoolMirroringImageHealth
      annotations:
        description: Mirroring image(s) (PV) in the pool {{ $labels.name }} are in
          Warning state for more than 1m. Mirroring might not work as expected in
          namespace:cluster {{ $labels.namespace }}:{{ $labels.managedBy }}.
        message: Mirroring image(s) (PV) in the pool {{ $labels.name }} are in Warning
          state for cluster {{ $labels.namespace }}:{{ $labels.managedBy }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/OdfPoolMirroringImageHealth.md
        severity_level: warning
        storage_type: ceph
      expr: |
        (ocs_pool_mirroring_image_health{job="ocs-metrics-exporter"}  * on (namespace, managedBy) group_left() (max by(namespace, managedBy) (ocs_pool_mirroring_status{job="ocs-metrics-exporter"}))) == 2
      for: 1m
      labels:
        mirroring_image_status: warning
        severity: warning
    - alert: OdfPoolMirroringImageHealth
      annotations:
        description: Mirroring image(s) (PV) in the pool {{ $labels.name }} are in
          Error state for more than 10s. Mirroring is not working as expected in namespace:cluster
          {{ $labels.namespace }}:{{ $labels.managedBy }}.
        message: Mirroring image(s) (PV) in the pool {{ $labels.name }} are in Error
          state for cluster {{ $labels.namespace }}:{{ $labels.managedBy }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/OdfPoolMirroringImageHealth.md
        severity_level: error
        storage_type: ceph
      expr: |
        (ocs_pool_mirroring_image_health{job="ocs-metrics-exporter"}  * on (namespace, managedBy) group_left() (max by(namespace, managedBy) (ocs_pool_mirroring_status{job="ocs-metrics-exporter"}))) == 3
      for: 10s
      labels:
        mirroring_image_status: error
        severity: critical
    - alert: ODFPersistentVolumeMirrorStatus
      annotations:
        description: Persistent volume {{ $labels.name }}/{{ $labels.namespace }}
          is not mirrored properly to peer site {{ $labels.site_name }} for more than
          1m. RBD image={{ $labels.image }} and CephBlockPool={{ $labels.pool_name
          }}. Please check namespace:cluster {{ $labels.namespace }}:{{ $labels.managedBy
          }}.
        message: Persistent volume {{ $labels.name }}/{{ $labels.namespace }} is not
          mirrored properly to peer site {{ $labels.site_name }} in namespace:cluster
          {{ $labels.namespace }}:{{ $labels.managedBy }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/ODFPersistentVolumeMirrorStatus.md
        severity_level: error
        storage_type: ceph
      expr: |
        ocs_rbd_mirror_image_state{job="ocs-metrics-exporter"} * on(image,pool_name) group_left(name,namespace,managedBy) ocs_rbd_pv_metadata{job="ocs-metrics-exporter"} == 1
      for: 1m
      labels:
        severity: critical
    - alert: ODFPersistentVolumeMirrorStatus
      annotations:
        description: Status unknown for Persistent volume {{ $labels.name }}/{{ $labels.namespace
          }} to peer site {{ $labels.site_name }} for more than 1m. RBD image={{ $labels.image
          }} and CephBlockPool={{ $labels.pool_name }}. Please check namespace:cluster
          {{ $labels.namespace }}:{{ $labels.managedBy }}
        message: Status unknown for Persistent volume {{ $labels.name }}/{{ $labels.namespace
          }} mirroring to peer site {{ $labels.site_name }} in namespace:cluster {{
          $labels.namespace }}:{{ $labels.managedBy }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/ODFPersistentVolumeMirrorStatus.md
        severity_level: warning
        storage_type: ceph
      expr: |
        ocs_rbd_mirror_image_state{job="ocs-metrics-exporter"} * on(image,pool_name) group_left(name,namespace,managedBy) ocs_rbd_pv_metadata{job="ocs-metrics-exporter"} == 0
      for: 1m
      labels:
        severity: warning
  - name: odf-obc-quota-alert.rules
    rules:
    - alert: ObcQuotaBytesAlert
      annotations:
        description: ObjectBucketClaim {{ $labels.objectbucketclaim }} has crossed
          80% of the size limit set by the quota(bytes) and will become read-only
          on reaching the quota limit in namespace:cluster {{ $labels.namespace }}:{{
          $labels.managedBy }}. Increase the quota in the {{ $labels.objectbucketclaim
          }} OBC custom resource.
        message: OBC has crossed 80% of the quota(bytes) in namespace:cluster {{ $labels.namespace
          }}:{{ $labels.managedBy }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/ObcQuotaBytesAlert.md
        severity_level: warning
        storage_type: RGW
      expr: |
        (ocs_objectbucketclaim_info * on (namespace, objectbucket, managedBy) group_left() (ocs_objectbucket_used_bytes/ocs_objectbucket_max_bytes)) > 0.80 < 1
      for: 10s
      labels:
        severity: warning
    - alert: ObcQuotaObjectsAlert
      annotations:
        description: ObjectBucketClaim {{ $labels.objectbucketclaim }} has crossed
          80% of the size limit set by the quota(objects) and will become read-only
          on reaching the quota limit in namespace:cluster {{ $labels.namespace }}:{{
          $labels.managedBy }}. Increase the quota in the {{ $labels.objectbucketclaim
          }} OBC custom resource.
        message: OBC has crossed 80% of the quota(object) in namespace:cluster {{
          $labels.namespace }}:{{ $labels.managedBy }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/ObcQuotaObjectsAlert.md
        severity_level: warning
        storage_type: RGW
      expr: |
        (ocs_objectbucketclaim_info * on (namespace, managedBy, objectbucket) group_left() (ocs_objectbucket_objects_total/ocs_objectbucket_max_objects)) > 0.80 < 1
      for: 10s
      labels:
        severity: warning
    - alert: ObcQuotaBytesExhausedAlert
      annotations:
        description: ObjectBucketClaim {{ $labels.objectbucketclaim }} has crossed
          the limit set by the quota(bytes) and will be read-only now in namespace:cluster
          {{ $labels.namespace }}:{{ $labels.managedBy }}. Increase the quota in the
          {{ $labels.objectbucketclaim }} OBC custom resource immediately.
        message: OBC reached quota(bytes) limit in namespace:cluster {{ $labels.namespace
          }}:{{ $labels.managedBy }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/ObcQuotaBytesExhausedAlert.md
        severity_level: error
        storage_type: RGW
      expr: |
        (ocs_objectbucketclaim_info * on (namespace, managedBy, objectbucket) group_left() (ocs_objectbucket_used_bytes/ocs_objectbucket_max_bytes)) >= 1
      for: 0s
      labels:
        severity: critical
    - alert: ObcQuotaObjectsExhausedAlert
      annotations:
        description: ObjectBucketClaim {{ $labels.objectbucketclaim }} has crossed
          the limit set by the quota(objects) and will be read-only now in namespace:cluster
          {{ $labels.namespace }}:{{ $labels.managedBy }}. Increase the quota in the
          {{ $labels.objectbucketclaim }} OBC custom resource immediately.
        message: OBC reached quota(object) limit in namespace:cluster {{ $labels.namespace
          }}:{{ $labels.managedBy }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/ObcQuotaObjectsExhausedAlert.md
        severity_level: error
        storage_type: RGW
      expr: |
        (ocs_objectbucketclaim_info * on (namespace, managedBy, objectbucket) group_left() (ocs_objectbucket_objects_total/ocs_objectbucket_max_objects)) >= 1
      for: 0s
      labels:
        severity: critical
  - name: cluster-services-alert.rules
    rules:
    - alert: ClusterObjectStoreState
      annotations:
        description: RGW endpoint of the Ceph object store is in a failure state or
          one or more Rook Ceph RGW deployments have fewer ready replicas than required
          for more than 15s. Please check the health of the Ceph cluster and the deployments
          in namespace:cluster {{ $labels.namespace }}:{{ $labels.managedBy }}.
        message: Cluster Object Store is in unhealthy state or number of ready replicas
          for Rook Ceph RGW deployments is less than the desired replicas in namespace:cluster
          {{ $labels.namespace }}:{{ $labels.managedBy }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/ClusterObjectStoreState.md
        severity_level: error
        storage_type: RGW
      expr: |
        ocs_rgw_health_status{job="ocs-metrics-exporter"} == 2
        or
        kube_deployment_status_replicas_ready{deployment=~"rook-ceph-rgw-.*"} < kube_deployment_spec_replicas{deployment=~"rook-ceph-rgw-.*"}
      for: 15s
      labels:
        severity: critical
  - name: ceph-blocklist-alerts.rules
    rules:
    - alert: ODFRBDClientBlocked
      annotations:
        description: An RBD client might be blocked by Ceph on node {{ $labels.node_name
          }} in namespace:cluster {{ $labels.namespace }}:{{ $labels.managedBy }}.
          This alert is triggered when the ocs_rbd_client_blocklisted metric reports
          a value of 1 for the node and there are pods in a CreateContainerError state
          on the node. This may cause the filesystem for the PVCs to be in a read-only
          state. Please check the pod description for more details.
        message: An RBD client might be blocked by Ceph on node {{ $labels.node_name
          }} in namespace:cluster {{ $labels.namespace }}:{{ $labels.managedBy }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/ODFRBDClientBlocked.md
        severity_level: error
      expr: |
        (
          ocs_rbd_client_blocklisted{node=~".+"} == 1
        )
        and on(node) (
          kube_pod_container_status_waiting_reason{reason="CreateContainerError"}
          * on(pod, namespace, managedBy) group_left(node)
          kube_pod_info
        ) > 0
      for: 10s
      labels:
        severity: warning
  - name: ocs-encryption-alert.rules
    rules:
    - alert: KMSServerConnectionAlert
      annotations:
        description: Storage Cluster KMS Server is in un-connected state for more
          than 5s. Please check KMS config in namespace:cluster {{ $labels.namespace
          }}:{{ $labels.managedBy }}.
        message: Storage Cluster KMS Server is in un-connected state. Please check
          KMS config in namespace:cluster {{ $labels.namespace }}:{{ $labels.managedBy
          }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/KMSServerConnectionAlert.md
        severity_level: error
        storage_type: ceph
      expr: |
        ocs_storagecluster_kms_connection_status{job="ocs-metrics-exporter"} == 1
      for: 5s
      labels:
        severity: critical
  - name: storage-client-alerts.rules
    rules:
    - alert: StorageClientHeartbeatMissed
      annotations:
        description: Storage Client ({{ $labels.storage_consumer_name }}) heartbeat missed for more than 120 (s). Lossy network connectivity might exist in namespace:cluster {{ $labels.namespace }}:{{ $labels.managedBy }}.
        message: Storage Client ({{ $labels.storage_consumer_name }}) heartbeat missed for more than 120 (s) in namespace:cluster {{ $labels.namespace }}:{{ $labels.managedBy }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/StorageClientHeartbeatMissed.md
        severity_level: warning
      expr: |
        (time() - 120) > (ocs_storage_client_last_heartbeat > 0)
      labels:
        severity: warning
    - alert: StorageClientHeartbeatMissed
      annotations:
        description: Storage Client ({{ $labels.storage_consumer_name }}) heartbeat missed for more than 300 (s). Client might have lost network connectivity in namespace:cluster {{ $labels.namespace }}:{{ $labels.managedBy }}.
        message: Storage Client ({{ $labels.storage_consumer_name }}) heartbeat missed for more than 300 (s) in namespace:cluster {{ $labels.namespace }}:{{ $labels.managedBy }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/StorageClientHeartbeatMissed.md
        severity_level: critical
      expr: |
        (time() - 300) > (ocs_storage_client_last_heartbeat > 0)
      labels:
        severity: critical
    - alert: StorageClientIncompatibleOperatorVersion
      annotations:
        description: Storage Client Operator ({{ $labels.storage_consumer_name }}) lags by 1 minor version. Client configuration may be incompatible in namespace:cluster {{ $labels.namespace }}:{{ $labels.managedBy }}.
        message: Storage Client Operator ({{ $labels.storage_consumer_name }}) lags by 1 minor version in namespace:cluster {{ $labels.namespace }}:{{ $labels.managedBy }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/StorageClientIncompatibleOperatorVersion.md
        severity_level: warning
      expr: |
        floor((ocs_storage_provider_operator_version>0)/1000) - ignoring(storage_consumer_name) group_right() floor((ocs_storage_client_operator_version>0)/1000) == 1
      labels:
        severity: warning
    - alert: StorageClientIncompatibleOperatorVersion
      annotations:
        description: Storage Client Operator ({{ $labels.storage_consumer_name }}) differs by more than 1 minor version. Client configuration may be incompatible and unsupported in namespace:cluster {{ $labels.namespace }}:{{ $labels.managedBy }}.
        message: Storage Client Operator ({{ $labels.storage_consumer_name }}) differs by more than 1 minor version in namespace:cluster {{ $labels.namespace }}:{{ $labels.managedBy }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/StorageClientIncompatibleOperatorVersion.md
        severity_level: critical
      expr: |
        floor((ocs_storage_provider_operator_version>0)/1000) - ignoring(storage_consumer_name) group_right() floor((ocs_storage_client_operator_version>0)/1000) > 1 or floor((ocs_storage_client_operator_version>0)/1000) - ignoring(storage_consumer_name) group_left() floor((ocs_storage_provider_operator_version>0)/1000) >= 1
      labels:
        severity: critical
  - name: storage-cluster-alerts.rules
    rules:
    - alert: CephMonLowNumber
      annotations:
        description: The number of failure zones available allow to increase the number of Ceph monitors from 3 to 5 in order to improve cluster resilience.
        message: The current number of Ceph monitors can be increased in order to improve cluster resilience.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephMonLowNumber.md
        severity_level: info
        storage_type: ceph
      expr: |
        ((count by (managedBy, namespace) (ceph_mon_metadata)<5) - on(managedBy,namespace) group_right() (ocs_storagecluster_failure_domain_count>=5)) < 0
      labels:
        severity: info
  - name: ceph-daemon-performance-alerts.rules
    rules:
    - alert: MDSCacheUsageHigh
      annotations:
        description: MDS cache usage for the daemon {{ $labels.ceph_daemon }} has
          exceeded above 95% of the requested value. Increase the memory request for
          {{ $labels.ceph_daemon }} pod.
        message: High MDS cache usage for the daemon {{ $labels.ceph_daemon }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephMdsCacheUsageHigh.md
        severity_level: error
      expr: |
        (ceph_mds_mem_rss * 1000) / on(ceph_daemon) group_left(job)(label_replace(kube_pod_container_resource_requests{container="mds", resource="memory"}, "ceph_daemon", "mds.$1", "pod", "rook-ceph-mds-(.*)-(.*)") * .5) > .95
      for: 5m
      labels:
        severity: critical
    - alert: OSDCPULoadHigh
      annotations:
        description: CPU usage for osd on pod {{ $labels.pod }} has exceeded 80%.
          Consider creating more OSDs to increase performance
        message: High CPU usage detected in OSD container on pod {{ $labels.pod}}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/OSDCPULoadHigh.md
        severity_level: warning
      expr: |
        pod:container_cpu_usage:sum{pod=~"rook-ceph-osd-.*"} / on(pod) kube_pod_resource_limit{resource='cpu',pod=~"rook-ceph-osd-.*"} > 0.80
      for: 30m
      labels:
        severity: warning
    - alert: MDSCPUUsageHigh
      annotations:
        description: |-
          Ceph metadata server pod ({{ $labels.pod }}) has high cpu usage.
          Please consider increasing the CPU request for the {{ $labels.pod }} pod as described in the runbook.
        message: Ceph metadata server pod ({{ $labels.pod }}) has high cpu usage
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/CephMdsCpuUsageHigh.md
        severity_level: warning
      expr: |
        pod:container_cpu_usage:sum{pod=~"rook-ceph-mds.*"}/ on(pod) kube_pod_resource_limit{resource='cpu',pod=~"rook-ceph-mds.*"} > 0.67
      for: 6h
      labels:
        severity: warning
    - alert: ODFClusterHasLegacyOSDs
      annotations:
        description: |-
          Ceph Cluster {{ $labels.ceph_cluster }} has legacy OSDs in the namespace {{ $labels.namespace }}.
          LVM-based OSDs on a PVC are deprecated, see documentation on replacing OSDs.
        message: Ceph Cluster {{ $labels.ceph_cluster}} has legacy OSDs
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/openshift-container-storage-operator/ODFClusterHasLegacyOSDs.md
        severity_level: warning
      expr: |
        ocs_lvm_osd_count > 0
      for: 1m
      labels:
        severity: warning
